%!TEX root = ../Report.tex

\section{Summary}
\label{section:conclusion_and_future_work:conclusion}

Previous work has demonstrated that contention between programs has performance implications, and that by considering their contention when scheduling, we can improve system throughput \cite{lira}. In this project, we have built upon this, by investigating the question: ``Can we gain more performance by exploiting both contention aware scheduling and plastic programming?''. As such, we have laid the groundwork for a contention aware plastic parallel programming library. We have demonstrated how such a library may be implemented, that the overhead incurred is acceptable, and that with two programs run in contention, it is possible to attain substantial gains in performance, with potential speedups from 1.03 to 2.44, compared to a non contention-aware system. We have also demonstrated that, with three programs run in contention, there are potential speedups of 1.13 to 2.43 compared to a non contention-aware system.

Our work towards a contention aware plastic parallel programming library shows promise, and provides exciting potential for further research, which are discussed in section \ref{section:conclusion_and_future_work:future_work}.





\section{Future Work}
\label{section:conclusion_and_future_work:future_work}

In this section, we discuss possible extensions to this project, which could become elements suitable for further study.



\subsection{More Detailed And Numerous Experiments}
\label{section:conclusion_and_future_work:more_detailed_and_numerous_experiments}

For many of our experiments, in particular the contention experiments, we had to cut down on their detail, and the overall number, due to time constraints. The existing framework easily allows us to run additional, increasingly detailed experiments, and a significant step would be to evaluate the potential of this system with real applications, as opposed to our purely synthetic workloads, so this is left as work for the future.



\subsection{More Plasticity}
\label{section:conclusion_and_future_work:more_plasticity}

Currently, the plasticity provided in the synthetic test program is comprised of the ability to change the number of threads used, as well as the virtual CPU cores they are pinned (restricted) to. One extension could be to add additional plastic features, and to evaluate the gain, if any, that they provide. As long as the program is still correct, (in that it produces the correct output), any plastic modification could be made. For example, we could vary the data structures used, or the pattern in which data is accessed (currently, just sequential).



\subsection{A Complete Contention Aware Plastic Programming Library}
\label{section:conclusion_and_future_work:a_complete_contention_aware_plastic_parallel_programming_library}

In the first year of this project \cite{me}, we demonstrated how a contention aware plastic programming library could be implemented, as well as showing that the overhead incurred was not significant. However, the pattern we implemented (map-array) did not show much room for improvement. In this second year, we have evaluated a more complex pattern, that of stencil codes, and specifically the Jacobi pattern, without integrating it into such a library. One natural extension would be to add this pattern to such a library, and possibly other common parallel programming patterns, such as the pipeline pattern, or a recursive pattern. Another natural extension would be to evaluate more workloads, such as workloads which stress the HDD, or perform many OS calls. A synthetic test program with multiple patterns and workloads could give substantial importent data.



\subsection{A Rudimentary Controller Application}
\label{section:conclusion_and_future_work:a_rudimentary_controller_application}

In the contention aware plastic parallel programming library we presented in the first year of this project \cite{me}, we employed a controller application to subsidise the communication and co-ordination between instances of our library. Because of the complexities of implementing the prototype, the controller application acted according to hand written rules for testing purposes, whereas, in a complete system, it would act according to input that it gathers about the current state of the system. Implementing such a controller application is a possible extension of this project.



\subsection{A More Advanced Controller Application}
\label{section:conclusion_and_future_work:a_more_advanced_controller_application}

In the previous extension, we would still be using hand written (albeit complex) rules to implement the controller application. It may be of some benefit to use some sort of artificial intelligence to figure out the optimal configuration. To generate the input data, we could use established methods of predicting performance, such as software profiling programs before hand, or hardware monitoring programs during execution. This is a well researched field, and here we present a short overview of these two methods:

\begin{itemize}
    \item Software profiling data involves collecting statistics such as the local IPC and L2 cache misses in a simulator. Data such as this can be used to predict the programs performance, and has been used to tune the compilation of a program for greater performance \cite{duesterwald_bala_2000}.
    
    \item Hardware monitoring requires keeping track of various statistics during program execution. In previous work involving hardware based profiling techniques, several statistics have been evaluated, such as IPC, floating point IPC, resource utilisation, and instruction dependencies \cite{bahar_manne_2001, buyuktosunoglu_schuster_brooks_bose_cook_albonesi_2001, folegnani_gonzalez, maro_bai_bahar_2001, ponomarev_kucuk_ghose, sasanka_hughes_adve_2002, seng_tune_tullsen}. All of these statistics could be used to inform the controller application, in particular, IPC has been shown to be a particularly useful statistic to monitor. The controller application would gather such statistics at regular intervals, and use it to inform its decisions. The advantage of this approach is that it gives data at runtime, allowing for the controller to dynamically adjust the overall configuration on the fly.
\end{itemize}



\subsection{Multiple Back-Ends And GPU Support}
\label{section:conclusion_and_future_work:multiple_backends_and_gpu_support}

Currently, our project is based in C++ and utilises pthreads as the parallel programming model. Additional support could be added for other approaches to parallelism (some of which are described in section \ref{section:background:current_solutions}), and even languages which target GPUs, such as OpenCL and CUDA. Providing this would allow the controller application to switch between them, as effectively another parameter that we can plastically change. The main decision however, would be between the CPU focused and the GPU focused approaches, as this would provide the most dramatic difference, and is dependent upon the characteristics of the workload in question.

GPUs have lots of compute cores, but with a much simpler architecture compared to a standard CPU core. This makes them powerful for floating point calculations in scientific computing. Utilising GPU oriented parallel programming approaches would introduce new challenges, such as explicitly managing data (copying back and forth from main memory to GPU memory), and restrictions imposed by the language. For example, in CUDA, device code (to be run on the GPU) must be C only, and cannot be called recursively. Another limitation is the lack of direct control of scheduling. GPUs have separate, efficient, thread managers which schedule threads, typically assigning many more threads than cores.



\subsection{Other Scheduler Metrics}
\label{section:conclusion_and_future_work:other_scheduler_metrics}

In this project, we investigated the utility of a contention aware plastic programming library with a focus on throughput as the metric to optimise. An extension to this project could investigate other metrics, such as:

\begin{itemize}
    \item \textbf{Resource Utilisation:} The percentage of time that a resource is busy
    \item \textbf{Turnaround Time:}      The amount of time between a process arriving ready to compute to the time it completes
    \item \textbf{Power Efficiency:}     The cost of compute power in terms of the energy used
\end{itemize}



\subsection{Other Computing Domains}
\label{section:conclusion_and_future_work:other_computing_domains}

As another extension, other computing domains could be considered.

\subsubsection{Mobile Systems}
\label{section:conclusion_and_future_work:mobile_systems}

The mobile computing domain introduces new challenges. Power efficiency is of key importance, and as such, this means that we can encounter diverse hardware. Many CPU cores is commonplace, as this leads to greater power efficiency. This makes parallel programming of key importance, in order to utilise the hardware efficiently. Another technique is to combine many low power CPU cores with few high power CPU cores, and even low power GPUs, to enable both efficiency and performance when required. Such features, and shifting our focus to power efficiency, would drastically change how we schedule applications.



\subsubsection{Server Systems}
\label{section:conclusion_and_future_work:server_systems}

A significant application of this technology is server systems with multiple parallel workloads. Taking this into account brings into question many possible extensions. Again, power efficiency is of key importance in many server systems, which typically prefer high core count systems. 

A more interesting challenge however would be supporting the use of distributed applications. This involves multiple machines coordinating on the same problem, and brings many questions. Should the controller application be distributed? Should controller applications even know about other nodes working on the same problem? Indeed, we may have a situation where one instance of a program on one node may use a different implementation to another instance on an entirely separate node, even though they are working on the same problem.



\section{Conclusion}

In conclusion, during this project, we have demonstrated that by exploiting contention aware scheduling and plasticity, we can attain substantial gains in performance in situations where programs are run in contention over and above the simple gains attributed to the introduction of parallelism. This lays the foundations for much potential future work.
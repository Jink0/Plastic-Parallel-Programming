%!TEX root = ../Report.tex
\section{}

***Overview of possible future work here, including work for next year and after that***

Currently optimizing for performance, can  optimize for other things e.g. energy efficiency for mobile applications etc.

Utilize GPUs, more exotic hardware.

Test on XXXII and other hardware.

further patterns left for possible future work. Producer consumer?

task stealing?

distributed system?

implementing some algorithm for automatic parameter tuning for future work.

Make work interruptable, e.g. if we assign static schedule we should still be able to change to other schedules

more experiments with varying memory access patterns

Vary Type of tasks (CPU Bottleneck/Memory Bottleneck) 

(*** schedule choice importance - Should the same be done for thread pinnings? And we have already kind of seen importance of number of threads, but should we have another experiment where num threads > num CPU cores?)

\section{Designing For Future Applications}

A real-world version of our library would include multiple common patterns of parallel programming, and may even utilize multiple backends allowing for different features (e.g. Standard Pthreads, OpenCL/CUDA for multi-GPU computation). It's feasible that the system could assess both the tasks presented and the environment (e.g. the particulars of the machine), and automatically allocate the resources of the machine so we perform in the most efficient manner.

This system would be useful in any performance orientated application, even when the machine will only be running a single instance, as we can still optimize the implementation to the environment on that machine. It would, however, come into it's own when we have multiple instances running simultaneously on a machine, a common situation with modern multiprogramming machines.

Could be used on multiple nodes in a distributed system

Another optimization would involve modifying the static schedule
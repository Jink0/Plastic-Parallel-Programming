%!TEX root = ../Report.tex
***Explain pattern implemented, (similar to OpenMP (Loop scheduling) and skepu, add differences to blackbourn's work) details of library (Controller etc), and how it may be used in a real system***

In this chapter, we will discuss the design of the system created to investigate this problem, and detail how such a system could be extended for the future.



\section{System description}

The ideas described in the background section are combined to produce a skeleton programming library with plasticity and contention aware scheduling. To keep it simple, we build the system incrementally, starting with a single parallel pattern, later adding plasticity and contention aware scheduling. 



\subsection{Skeleton Foundation}

As discussed in the background section, one of the key ides behind the project is that of skeleton programming, using predefined patterns to aid the programmer. The first pattern implemented is the map array pattern, with further patterns left for possible future work. Map-array is similar to the map pattern, in that it applies a given user function to each element in a list, however map-array also allows the function to access a user provided array. This was chosen as the map pattern is likely the most well known pattern and certainly one of the most useful, and map-array provides further functionality on top of this. It also provides a good basis for developing further patterns, and it also allows complex testing, which will be covered in the evaluation section of this report.

The parallel backend that the system is based upon is Pthreads due to it's wide availability, and the level of fine control allows us to tune all parameters of the program and implement functions which are not possible with other solutions, e.g. detailed metric analysis. 

Any dynamic task scheduling method will entail some overhead as a result of parcelling out tasks. 

The best choice for how much computation constitutes a task will be based on the computation to be done as well as the number of threads and other resources available at execution time.

Load balancing an application workload among threads is critical to performance. The key objective for load balancing is to minimize idle time on threads. Sharing the workload equally across all threads with minimal work sharing overheads results in fewer cycles wasted with idle threads not advancing the computation, and thereby leads to improved performance. However, achieving perfect load balance is non-trivial, and it depends on the parallelism within the application, workload, the number of threads, load balancing policy, and the threading implementation.





\subsection{Adding Plasticity}

To implement plasticity, we add the ability to vary three key aspects of the implementation of map-array:

\begin{itemize}
	\item Thread count - The number of threads we split the tasks between
	\item Thread pinnings - The particular CPU core each thread runs on
	\item Schedule - How to divide tasks between threads
\end{itemize}

similar to OpenMP (Loop scheduling)

***THREAD PINNING!***

Once we have added plasticity, we can experiment with the specifics of an implementation, and see how they affect the performance of the system. This would be the use case of utilizing our library with no other program running, (So no contention aware scheduling), and we can explore how we can adapt the program using plasticity at runtime in this case. We may be able to improve performance even under these conditions, depending upon the configuration of the the machine (e.g., is there more CPU cores available) and the problem (e.g. do we have many small tasks or few large tasks.) 



\subsection{Contention Aware Scheduling}

In order to explore situations with two parallel programs running simultaneously, our library needs to be able to communicate with other instances, and adapt it's behaviour accordingly. It would also be convenient to have a single place from which we control the aspects of the programs. To this end, our implementation includes a separate controller application, whose job it is to manage the parallel programs running on the machine using our library. With a separate controller, we can easily provide a single, known point of contact for the parallel programs, and compute the individual program parameters with respect to all aspects of the system.





\subsection{Evaluation}

To properly evaluate the outcome of this project, we also need points of comparison in terms of performance. So in addition to our skeleton library and controller application, we also implement a dummy program which can run the library with several different parameters to perform experiments. a sequential and an OpenMP version, in which we can vary similar parameters and produce comparable statistics. (*** If complete, add that they utilize the same testing framework)



\section{Real-World Applications}

A real-world version of our library would include multiple common patterns of parallel programming, and may even utilize multiple backends allowing for different features (e.g. Standard Pthreads, OpenCL/CUDA for multi-GPU computation). It's feasible that the system could asses both the tasks presented and the environment (e.g. the particulars of the machine), and automatically allocate the resources of the machine so we perform in the most efficient manner.

This system would be useful in any performance orientated application, even when the machine will only be running a single instance, as we can still optimize the implementation to the environment on that machine. It would, however, come into it's own when we have multiple instances running simultaneously on a machine, a common situation with modern multiprogramming machines.

(Could be used on multiple nodes in a distributed system?)

Different schedules

As an example of how this would work in practice, see the following flow diagram:

\begin{center}
\vspace{0.5cm}
\includegraphics[width=1\textwidth]{system_design_high_level_flow_diagram}
\end{center}

NOTE - CHANGE THIS GRAPH TO REFLECT FINAL IMPLEMENTATION
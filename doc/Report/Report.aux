\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{concurrency_revolution}
\citation{free_lunch}
\citation{parallel_challenges}
\citation{sequential_to_parallel}
\citation{petabricks}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{7}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:introduction}{{1}{7}{Introduction}{chapter.1}{}}
\citation{lira}
\citation{lira}
\citation{posix_threads}
\citation{mpi}
\citation{openmp_home}
\citation{openmp}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{9}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:background}{{2}{9}{Background}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Current Solutions}{9}{section.2.1}}
\citation{mpi}
\citation{lira}
\citation{lira}
\citation{lira}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}What Is Contention Aware Scheduling?}{10}{section.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Taken from the LIRA paper \cite  {lira}: Pair-wise speedup of programs, comparing sharing a socket to using separate sockets. Boxes annotated with a $-$ indicate cases where performance decreased, and $+$ where performance increased. In LIRA, this information was used to select which programs, from a given workset, should be scheduled concurrently, in order to maximize performance.\relax }}{11}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:lira_pairwise_speedup}{{2.1}{11}{Taken from the LIRA paper \cite {lira}: Pair-wise speedup of programs, comparing sharing a socket to using separate sockets. Boxes annotated with a $-$ indicate cases where performance decreased, and $+$ where performance increased. In LIRA, this information was used to select which programs, from a given workset, should be scheduled concurrently, in order to maximize performance.\relax }{figure.caption.2}{}}
\citation{petabricks}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}What Is Plastic Programming?}{12}{section.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A graph showing three algorithms with different runtime curves, which depend upon the array size. Combining these algorithms would provide an improved algorithm, with D1 and D2 showing the optimal decision points where a plastic programming system should switch algorithms\relax }}{13}{figure.caption.3}}
\newlabel{fig:plastic_graph}{{2.2}{13}{A graph showing three algorithms with different runtime curves, which depend upon the array size. Combining these algorithms would provide an improved algorithm, with D1 and D2 showing the optimal decision points where a plastic programming system should switch algorithms\relax }{figure.caption.3}{}}
\citation{patterns_and_frameworks}
\citation{skepu}
\citation{muesli}
\citation{lira}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}What Is Skeleton Programming?}{14}{section.2.4}}
\newlabel{section:background_what_is_skeleton_programming}{{2.4}{14}{What Is Skeleton Programming?}{section.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Summary}{14}{section.2.5}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Design}{17}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:design}{{3}{17}{Design}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}System description}{17}{section.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Skeleton Foundation}{17}{subsection.3.1.1}}
\newlabel{section:design_skeleton_foundation}{{3.1.1}{17}{Skeleton Foundation}{subsection.3.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Taken from SkePU documentation: Examples of patterns\relax }}{18}{figure.caption.4}}
\newlabel{fig:skepu_skeletons}{{3.1}{18}{Taken from SkePU documentation: Examples of patterns\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Adding Plasticity}{18}{subsection.3.1.2}}
\newlabel{subsection:design_adding_plasticity}{{3.1.2}{18}{Adding Plasticity}{subsection.3.1.2}{}}
\citation{lira}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces A worst case scenario of a static schedule assigning each thread an equal number of tasks. Suppose we have a set of twelve independent tasks with the following set of execution times (These would be unknown to the scheduler): \{10, 6, 4, 4, 2, 2, 2, 2, 1, 1, 1, 1\}. With four threads, a simple division of tasks would be three tasks each distributed in order. This figure illustrates this distribution of work. Note that the total execution time is 20 time units.\relax }}{20}{figure.caption.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces An optimized version of the previous schedule. Here we have the same set of tasks, but we have a better distribution of work, so the total execution time is 10 time units.\relax }}{20}{figure.caption.5}}
\newlabel{fig:schedule_optimization}{{3.3}{20}{An optimized version of the previous schedule. Here we have the same set of tasks, but we have a better distribution of work, so the total execution time is 10 time units.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Contention Aware Scheduling}{21}{subsection.3.1.3}}
\newlabel{subsection:design_contention_aware_scheduling}{{3.1.3}{21}{Contention Aware Scheduling}{subsection.3.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces High level communication model of the system, with an arbitrary number of programs, with an arbitrary number of threads. Two way communication occurs between the controller and each main thread, and then between each main thread and it's worker threads.\relax }}{22}{figure.caption.6}}
\newlabel{fig:communication_structure}{{3.4}{22}{High level communication model of the system, with an arbitrary number of programs, with an arbitrary number of threads. Two way communication occurs between the controller and each main thread, and then between each main thread and it's worker threads.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces An example of how contention aware scheduling can be enhanced with plastic programming. We have two programs, represented by different colours. As time progresses, we see that when program 2 launches, we have a decision point. Here, the system decides how many resources to give each program and what implementation they should use, according to what is optimal (In this project, we do this manually.)     Moving further in time, we have another decision point. This one is triggered by some other change in the machine, which means the optimal configuration has changed. So the system updates the configuration of each program, and continues.     Program 2 then terminates, triggering another decision point, and the system again updates the configuration.\relax }}{23}{figure.caption.7}}
\newlabel{fig:plastic_contention_aware_scheduling}{{3.5}{23}{An example of how contention aware scheduling can be enhanced with plastic programming. We have two programs, represented by different colours. As time progresses, we see that when program 2 launches, we have a decision point. Here, the system decides how many resources to give each program and what implementation they should use, according to what is optimal (In this project, we do this manually.) \\ \\ Moving further in time, we have another decision point. This one is triggered by some other change in the machine, which means the optimal configuration has changed. So the system updates the configuration of each program, and continues. \\ \\ Program 2 then terminates, triggering another decision point, and the system again updates the configuration.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Evaluation}{24}{subsection.3.1.4}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Implementation}{25}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:implementation}{{4}{25}{Implementation}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Skeleton Foundation}{25}{section.4.1}}
\newlabel{section:implementation_skeleton_foundation}{{4.1}{25}{Skeleton Foundation}{section.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Interface of our map\_array skeleton. The first four variables are the two input arrays, the function to apply, and the output array respectively. The output\_filename variable is the filename to record the metrics output in, and params sets up the initial parameters we will use. These last two are optional.\relax }}{26}{figure.caption.8}}
\newlabel{fig:implementation_map_array_interface}{{4.1}{26}{Interface of our map\_array skeleton. The first four variables are the two input arrays, the function to apply, and the output array respectively. The output\_filename variable is the filename to record the metrics output in, and params sets up the initial parameters we will use. These last two are optional.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces A usage example of map\_array, here we apply our user\_function to each element of input1. The size of our two input arrays need not match, but the size of the input1 and output arrays must.\relax }}{26}{figure.caption.9}}
\newlabel{fig:implementation_map_array_usage_example}{{4.2}{26}{A usage example of map\_array, here we apply our user\_function to each element of input1. The size of our two input arrays need not match, but the size of the input1 and output arrays must.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces The parallel portion of our map\_array skeleton. This figure shows how we use Pthreads to create our worker threads, providing each with access to it's own thread data object.\relax }}{28}{figure.caption.10}}
\newlabel{fig:implementation_create_threads}{{4.3}{28}{The parallel portion of our map\_array skeleton. This figure shows how we use Pthreads to create our worker threads, providing each with access to it's own thread data object.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Function which joins with the specified number of threads. Starts with the last thread to be created, so that if we wish to terminate threads, the threads which remain executing preserve their logical ordering.\relax }}{29}{figure.caption.11}}
\newlabel{fig:implementation_join_threads}{{4.4}{29}{Function which joins with the specified number of threads. Starts with the last thread to be created, so that if we wish to terminate threads, the threads which remain executing preserve their logical ordering.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Adding Plasticity}{29}{section.4.2}}
\newlabel{section:implementation_adding_plasticity}{{4.2}{29}{Adding Plasticity}{section.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces An example of how the main thread handles a switch in the implementation.\relax }}{31}{figure.caption.12}}
\newlabel{fig:implementation_main_thread_bot_comms}{{4.5}{31}{An example of how the main thread handles a switch in the implementation.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces The inter-thread communication sections of the bag of tasks object. ``thread\_control'' provides a control variable for each thread, which the thread reads and reacts to. The ``empty'' boolean is updated when the getTasks method is run, and is read periodically by the main thread to check if we have completed the computation.\relax }}{32}{figure.caption.13}}
\newlabel{fig:implementation_bot_comms}{{4.6}{32}{The inter-thread communication sections of the bag of tasks object. ``thread\_control'' provides a control variable for each thread, which the thread reads and reacts to. The ``empty'' boolean is updated when the getTasks method is run, and is read periodically by the main thread to check if we have completed the computation.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Function for sticking the current thread to a particular CPU core. Enables thread pinning plasticity.\relax }}{33}{figure.caption.14}}
\newlabel{fig:implementation_thread_pinning}{{4.7}{33}{Function for sticking the current thread to a particular CPU core. Enables thread pinning plasticity.\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Contention Aware Scheduling}{34}{section.4.3}}
\newlabel{section:implementation_contention_aware_scheduling}{{4.3}{34}{Contention Aware Scheduling}{section.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Communication model for communications between applications and the controller. Thin lines represent program flow, thick lines represent inter-process messages.\relax }}{35}{figure.caption.15}}
\newlabel{fig:controller_flowchart}{{4.8}{35}{Communication model for communications between applications and the controller. Thin lines represent program flow, thick lines represent inter-process messages.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces An example of how the main thread implements a change to the parameters received from the controller. Demonstrates the usage of the ZeroMQ library to provide inter-process communication with the controller.\relax }}{36}{figure.caption.16}}
\newlabel{fig:implementation_controller_comms}{{4.9}{36}{An example of how the main thread implements a change to the parameters received from the controller. Demonstrates the usage of the ZeroMQ library to provide inter-process communication with the controller.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Testing Programs}{37}{section.4.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Utility Code}{37}{section.4.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces A thread safe print function. Prints a variable type of and a variable number of arguments to an output stream. Implementation is split accross .hpp and .cpp files, as the need for templates requires that certain functions must be defined in the header, so the compiler can see them during linking.\relax }}{38}{figure.caption.17}}
\newlabel{fig:implementation_thread_safe_print}{{4.10}{38}{A thread safe print function. Prints a variable type of and a variable number of arguments to an output stream. Implementation is split accross .hpp and .cpp files, as the need for templates requires that certain functions must be defined in the header, so the compiler can see them during linking.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces A function for executing an external program, used in the setup for experiment 5.\relax }}{39}{figure.caption.18}}
\newlabel{fig:implementation_execute_external_program}{{4.11}{39}{A function for executing an external program, used in the setup for experiment 5.\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Experimental Methodology And Program}{41}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:experimental_methodology_and_program}{{5}{41}{Experimental Methodology And Program}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Evaluation Methodology}{41}{section.5.1}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Programme of Experiments}{42}{section.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Experiment 1 - Metrics Collection Overhead}{43}{subsection.5.2.1}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Experiment 1 Parameters\relax }}{44}{table.caption.19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Experiment 2 - Absolute Performance}{44}{subsection.5.2.2}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Experiment 2 Parameters\relax }}{45}{table.caption.20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Experiment 3 - Plasticity And Contention Aware Scheduling Framework Overhead}{45}{subsection.5.2.3}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Experiment 3 Parameters\relax }}{45}{table.caption.21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}Experiment 4 - Schedule Choice Importance}{46}{subsection.5.2.4}}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces Experiment 4 Parameters\relax }}{46}{table.caption.22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.5}Experiment 5 - Absolute Multiprogramming Performance}{47}{subsection.5.2.5}}
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces Experiment 5 Parameters\relax }}{47}{table.caption.23}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Results}{49}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:results}{{6}{49}{Results}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Experiment Results}{49}{section.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Experiment 1 - Metrics Collection Overhead}{49}{subsection.6.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Experiment 1 results\relax }}{50}{figure.caption.24}}
\newlabel{fig:results_ex1}{{6.1}{50}{Experiment 1 results\relax }{figure.caption.24}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Experiment 1 Results Analysis\relax }}{50}{table.caption.25}}
\newlabel{table:results_experiment_1_results_analysis}{{6.1}{50}{Experiment 1 Results Analysis\relax }{table.caption.25}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Experiment 1 Parameters\relax }}{50}{table.caption.26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Experiment 2 - Absolute Performance}{51}{subsection.6.1.2}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Experiment 2 Parameters\relax }}{51}{table.caption.28}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Experiment 2 results\relax }}{52}{figure.caption.27}}
\newlabel{fig:results_ex2}{{6.2}{52}{Experiment 2 results\relax }{figure.caption.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}Experiment 3 - Plasticity And Contention Aware Scheduling Framework Overhead}{53}{subsection.6.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Experiment 3 results\relax }}{54}{figure.caption.29}}
\newlabel{fig:results_ex3}{{6.3}{54}{Experiment 3 results\relax }{figure.caption.29}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces Experiment 3 Results Analysis\relax }}{54}{table.caption.30}}
\newlabel{table:results_experiment_3_results_analysis}{{6.4}{54}{Experiment 3 Results Analysis\relax }{table.caption.30}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.5}{\ignorespaces Experiment 3 Parameters\relax }}{54}{table.caption.31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.4}Experiment 4 - Schedule Choice Importance}{55}{subsection.6.1.4}}
\@writefile{lot}{\contentsline {table}{\numberline {6.6}{\ignorespaces Experiment 4 Parameters\relax }}{55}{table.caption.33}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Experiment 4 results\relax }}{56}{figure.caption.32}}
\newlabel{fig:results_ex4}{{6.4}{56}{Experiment 4 results\relax }{figure.caption.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.5}Experiment 5 - Absolute Multiprogramming Performance}{57}{subsection.6.1.5}}
\@writefile{lot}{\contentsline {table}{\numberline {6.7}{\ignorespaces Experiment 5 Parameters\relax }}{57}{table.caption.35}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Experiment 5 results\relax }}{58}{figure.caption.34}}
\newlabel{fig:results_ex5}{{6.5}{58}{Experiment 5 results\relax }{figure.caption.34}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Conclusion}{59}{section.6.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Future Work And Conclusions}{61}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:future_work_and_conclusions}{{7}{61}{Future Work And Conclusions}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Future Development}{61}{section.7.1}}
\citation{datacentres}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Personal Reflection}{63}{subsection.7.1.1}}
\bibstyle{acm}
\bibdata{references}
\bibcite{free_lunch}{1}
\bibcite{muesli}{2}
\bibcite{petabricks}{3}
\bibcite{posix_threads}{4}
\bibcite{patterns_and_frameworks}{5}
\bibcite{openmp}{6}
\bibcite{lira}{7}
\bibcite{skepu}{8}
\bibcite{openmp_home}{9}
\bibcite{datacentres}{10}
\bibcite{parallel_challenges}{11}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Bibliography}{65}{chapter.8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{sequential_to_parallel}{12}
\bibcite{mpi}{13}
\bibcite{concurrency_revolution}{14}

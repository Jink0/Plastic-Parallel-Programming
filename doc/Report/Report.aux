\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{parallel_challenges}
\citation{petabricks}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{4}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:introduction}{{1}{4}{Introduction}{chapter.1}{}}
\citation{lira}
\citation{lira}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{6}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:background}{{2}{6}{Background}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Current Solutions}{6}{section.2.1}}
\citation{mpi}
\citation{lira}
\citation{lira}
\citation{lira}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}What Is Contention Aware Scheduling?}{7}{section.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Taken from the LIRA paper \cite  {lira}: Pair-wise speedup of programs, comparing sharing a socket to using separate sockets. Boxes annotated with a $-$ indicate cases where performance decreased, and $+$ where performance increased. In LIRA, this information was used to select which programs, from a given workset, should be scheduled concurrently, in order to maximize performance.\relax }}{8}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:lira_pairwise_speedup}{{2.1}{8}{Taken from the LIRA paper \cite {lira}: Pair-wise speedup of programs, comparing sharing a socket to using separate sockets. Boxes annotated with a $-$ indicate cases where performance decreased, and $+$ where performance increased. In LIRA, this information was used to select which programs, from a given workset, should be scheduled concurrently, in order to maximize performance.\relax }{figure.caption.2}{}}
\citation{petabricks}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}What Is Plastic Programming?}{9}{section.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A graph showing three algorithms with different runtime curves, which depend upon the array size. Combining these algorithms would provide an improved algorithm, with D1 and D2 showing the optimal decision points where a plastic programming system should switch algorithms\relax }}{10}{figure.caption.3}}
\newlabel{fig:plastic_graph}{{2.2}{10}{A graph showing three algorithms with different runtime curves, which depend upon the array size. Combining these algorithms would provide an improved algorithm, with D1 and D2 showing the optimal decision points where a plastic programming system should switch algorithms\relax }{figure.caption.3}{}}
\citation{lira}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}What Is Skeleton Programming?}{11}{section.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Summary}{11}{section.2.5}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Design}{13}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:design}{{3}{13}{Design}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}System description}{13}{section.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Skeleton Foundation}{13}{subsection.3.1.1}}
\newlabel{section:design_skeleton_foundation}{{3.1.1}{13}{Skeleton Foundation}{subsection.3.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Taken from SkePU documentation: Examples of skeletons\relax }}{14}{figure.caption.4}}
\newlabel{fig:skepu_skeletons}{{3.1}{14}{Taken from SkePU documentation: Examples of skeletons\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Adding Plasticity}{14}{subsection.3.1.2}}
\newlabel{subsection:design_adding_plasticity}{{3.1.2}{14}{Adding Plasticity}{subsection.3.1.2}{}}
\citation{lira}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces A worst case scenario of a static schedule assigning each thread an equal number of tasks. Suppose we have a set of twelve independent tasks with the following set of execution times (These would be unknown to the scheduler): \{10, 6, 4, 4, 2, 2, 2, 2, 1, 1, 1, 1\}. With four threads, a simple division of tasks would be three tasks each distributed in order. This figure illustrates this distribution of work. Note that the total execution time is 20 time units.\relax }}{16}{figure.caption.5}}
\newlabel{fig:unoptimized_schedule}{{3.2}{16}{A worst case scenario of a static schedule assigning each thread an equal number of tasks. Suppose we have a set of twelve independent tasks with the following set of execution times (These would be unknown to the scheduler): \{10, 6, 4, 4, 2, 2, 2, 2, 1, 1, 1, 1\}. With four threads, a simple division of tasks would be three tasks each distributed in order. This figure illustrates this distribution of work. Note that the total execution time is 20 time units.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces An optimized version of figure \ref  {fig:unoptimized_schedule}. Here we have the same set of tasks, but we have a better distribution of work, so the total execution time is 10 time units.\relax }}{17}{figure.caption.6}}
\newlabel{fig:optimized_schedule}{{3.3}{17}{An optimized version of figure \ref {fig:unoptimized_schedule}. Here we have the same set of tasks, but we have a better distribution of work, so the total execution time is 10 time units.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Contention Aware Scheduling}{18}{subsection.3.1.3}}
\newlabel{subsection:design_contention_aware_scheduling}{{3.1.3}{18}{Contention Aware Scheduling}{subsection.3.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Communication model for communications between applications and the controller. Thin lines represent program flow, thick lines represent inter-process messages.\relax }}{19}{figure.caption.7}}
\newlabel{fig:controller_flowchart}{{3.4}{19}{Communication model for communications between applications and the controller. Thin lines represent program flow, thick lines represent inter-process messages.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces An example of how contention aware scheduling can be enhanced with plastic programming. We have two programs, represented by different colours. As time progresses, we see that when program 2 launches, we have a decision point. Here, the system decides how many resources to give each program and what implementation they should use, according to what is optimal (In this project, we do this manually.)     Moving further in time, we have another decision point. This one is triggered by some change in the machine, which means the optimal configuration has changed. So the system updates the configuration of each program, and continues.     Program 2 then terminates, triggering another decision point, and the system again updates the configuration.\relax }}{20}{figure.caption.8}}
\newlabel{fig:plastic_contention_aware_scheduling}{{3.5}{20}{An example of how contention aware scheduling can be enhanced with plastic programming. We have two programs, represented by different colours. As time progresses, we see that when program 2 launches, we have a decision point. Here, the system decides how many resources to give each program and what implementation they should use, according to what is optimal (In this project, we do this manually.) \\ \\ Moving further in time, we have another decision point. This one is triggered by some change in the machine, which means the optimal configuration has changed. So the system updates the configuration of each program, and continues. \\ \\ Program 2 then terminates, triggering another decision point, and the system again updates the configuration.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Evaluation}{21}{subsection.3.1.4}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Implementation}{22}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:implementation}{{4}{22}{Implementation}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Skeleton Foundation}{22}{section.4.1}}
\newlabel{section:implementation_skeleton_foundation}{{4.1}{22}{Skeleton Foundation}{section.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Interface of our map\_array skeleton. The first four variables are the two input arrays, the function to apply, and the output array respectively. The output\_filename variable is the filename to record the metrics output in, and params sets up the initial parameters we will use. These last two are optional.\relax }}{23}{figure.caption.9}}
\newlabel{fig:map_array_interface}{{4.1}{23}{Interface of our map\_array skeleton. The first four variables are the two input arrays, the function to apply, and the output array respectively. The output\_filename variable is the filename to record the metrics output in, and params sets up the initial parameters we will use. These last two are optional.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces A usage example of map\_array, here we apply our user\_function to each element of input1. The size of our two input arrays need not match, but the size of the input1 and output arrays must.\relax }}{23}{figure.caption.10}}
\newlabel{fig:map_array_usage_example}{{4.2}{23}{A usage example of map\_array, here we apply our user\_function to each element of input1. The size of our two input arrays need not match, but the size of the input1 and output arrays must.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Adding Plasticity}{24}{section.4.2}}
\newlabel{section:implementation_adding_plasticity}{{4.2}{24}{Adding Plasticity}{section.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Contention Aware Scheduling}{25}{section.4.3}}
\newlabel{section:implementation_contention_aware_scheduling}{{4.3}{25}{Contention Aware Scheduling}{section.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Overall communication model of the system, with an arbitrary number of programs, with an arbitrary number of threads. Two way communication occurs between the controller and each main thread, and then between each main thread and it's worker threads.\relax }}{27}{figure.caption.11}}
\newlabel{fig:communication_structure}{{4.3}{27}{Overall communication model of the system, with an arbitrary number of programs, with an arbitrary number of threads. Two way communication occurs between the controller and each main thread, and then between each main thread and it's worker threads.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Testing Programs}{28}{section.4.4}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Experimental Methodology And Program}{29}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:experimental_methodology_and_program}{{5}{29}{Experimental Methodology And Program}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Evaluation Methodology}{29}{section.5.1}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Programme of Experiments}{30}{section.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Experiment 1 - Metrics Collection Overhead}{31}{subsection.5.2.1}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Experiment 1 Parameters\relax }}{32}{table.caption.12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Experiment 2 - Absolute Performance}{32}{subsection.5.2.2}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Experiment 2 Parameters\relax }}{33}{table.caption.13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Experiment 3 - Plasticity And Contention Aware Scheduling Framework Overhead}{33}{subsection.5.2.3}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Experiment 3 Parameters\relax }}{33}{table.caption.14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}Experiment 4 - Schedule Choice Importance}{34}{subsection.5.2.4}}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces Experiment 4 Parameters\relax }}{34}{table.caption.15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.5}Experiment 5 - Absolute Multiprogramming Performance}{35}{subsection.5.2.5}}
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces Experiment 5 Parameters\relax }}{35}{table.caption.16}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Results}{36}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:results}{{6}{36}{Results}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Experiment Results}{36}{section.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Experiment 1 - Metrics Collection Overhead}{36}{subsection.6.1.1}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Experiment 1 Parameters\relax }}{36}{table.caption.18}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Experiment 1 results\relax }}{37}{figure.caption.17}}
\newlabel{fig:results_ex1}{{6.1}{37}{Experiment 1 results\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Experiment 2 - Absolute Performance}{37}{subsection.6.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Experiment 2 results\relax }}{38}{figure.caption.19}}
\newlabel{fig:results_ex2}{{6.2}{38}{Experiment 2 results\relax }{figure.caption.19}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Experiment 2 Parameters\relax }}{38}{table.caption.20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}Experiment 3 - Plasticity And Contention Aware Scheduling Framework Overhead}{39}{subsection.6.1.3}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Experiment 3 Parameters\relax }}{39}{table.caption.22}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Experiment 3 results\relax }}{40}{figure.caption.21}}
\newlabel{fig:results_ex3}{{6.3}{40}{Experiment 3 results\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.4}Experiment 4 - Schedule Choice Importance}{41}{subsection.6.1.4}}
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces Experiment 4 Parameters\relax }}{41}{table.caption.24}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Experiment 4 results\relax }}{42}{figure.caption.23}}
\newlabel{fig:results_ex4}{{6.4}{42}{Experiment 4 results\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.5}Experiment 5 - Absolute Multiprogramming Performance}{43}{subsection.6.1.5}}
\@writefile{lot}{\contentsline {table}{\numberline {6.5}{\ignorespaces Experiment 5 Parameters\relax }}{43}{table.caption.25}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Discussion}{44}{section.6.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Future Work}{45}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:future_work}{{7}{45}{Future Work}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}}{45}{section.7.1}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Designing For Future Applications}{46}{section.7.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Conclusion}{47}{chapter.8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:conclusion}{{8}{47}{Conclusion}{chapter.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}}{47}{section.8.1}}
\bibstyle{acm}
\bibdata{references}
\bibcite{petabricks}{1}
\bibcite{lira}{2}
\bibcite{parallel_challenges}{3}
\bibcite{mpi}{4}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Bibliography}{48}{chapter.9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
